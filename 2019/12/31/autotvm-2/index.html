<!DOCTYPE html>
<html lang="zh-cn">
  <head><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="description" content="AutoTVM 探秘（二）"/><meta name="keywords" content="AutoTVM, TVM, Reku" /><link rel="alternate" href="/default" title="Reku" ><link rel="shortcut icon" type="image/x-icon" href="https://gitee.com/reku1997/reku1997/raw/master/reku.ico?v=2.11.0" />
<link rel="canonical" href="https://reku1997.gitee.io/2019/12/31/autotvm-2/"/>

<link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css" /><script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.11.0" />

<script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
<script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "MWLzM550UOu69h3dgvbbLSsF-gzGzoHsz",
      appKey: "gkKnwm9FK0cu3ysJbcggsCDz"
    });
  </script><script>
  window.config = {"leancloud":{"app_id":"MWLzM550UOu69h3dgvbbLSsF-gzGzoHsz","app_key":"gkKnwm9FK0cu3ysJbcggsCDz"},"toc":true,"fancybox":true,"pjax":"","latex":true};
</script>

    <title>AutoTVM 探秘（二） - Reku</title>
  <meta name="generator" content="Hexo 5.2.0"></head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">Reku</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list"><a href="/">
        <li class="mobile-menu-item">Home
          </li>
      </a><a href="/archives/">
        <li class="mobile-menu-item">Archives
          </li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags
          </li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About
          </li>
      </a></ul>
</nav>
<div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Reku</a>
</div>

<nav class="site-navbar"><ul id="menu" class="menu"><li class="menu-item">
          <a class="menu-item-link" href="/">
            Home
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            Archives
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/tags/">
            Tags
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/about/">
            About
            </a>
        </li>
      </ul></nav>
</header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content"><article class="post">
    <header class="post-header">
      <h1 class="post-title">AutoTVM 探秘（二）
        </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-12-31
        </span><span class="post-category">
            <a href="/categories/system/">system</a>
            <a href="/categories/system/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
        <span class="post-visits"
             data-url="/2019/12/31/autotvm-2/"
             data-title="AutoTVM 探秘（二）">
          Visits 0
        </span>
        </div>
    </header>

    <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">Contents</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="toc-text">问题定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2%E6%A1%86%E6%9E%B6"><span class="toc-text">搜索框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96"><span class="toc-text">贝叶斯优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="toc-text">迁移学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E6%95%88%E6%9E%9C"><span class="toc-text">实验效果</span></a></li></ol>
    </div>
  </div><div class="post-content"><p>好了，本篇开始进入正题！内容基本都来自于：[Learning to Optimize Tensor Programs. NeurlPS`18](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.08166">https://arxiv.org/abs/1805.08166</a>)</p>
<a id="more"></a>

<h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p><a href="https://reku1997.gitee.io/2019/12/30/autotvm-1/">上一篇文章</a>讲了 AutoTVM 的大致问题，现在给出数学上面的描述。 首先有一个 \(\mathcal{E}\) 代表所有可能的计算，\(e\in\mathcal{E}\) 就是我们要去优化的计算。对于这个 \(e\) 来说，有一个合法的 schedule space 叫 \(\mathcal{S}_e\)，其中每个合法的 schedule 就叫 \(s\in\mathcal{S}_e\)。\(x=g(e,s)\) 是 \(e\) 跟 \(s\) 通过编译器 \(g\) 生成的 low-level code，\(f(x)\) 为这个 \(x\) 在硬件上面实际运行的时间，那么该问题的定义则变成：\[\underset{s\in\mathcal{S}_e}{\operatorname{argmin}}f(g(e,s))\] </p>
<p>这个问题跟现在很多人研究的 hyper-parameter optimization 非常相似，然而 paper 的作者认为，该问题跟传统的 hyper-parameter optimization 有以下几个区别： 第一个区别就是 tensor optimization 比传统的 hyper-parameter optimization 要快很多。因为超参数搜索优化的目标是神经网络的效果，所以训练一次其实是非常慢的，所以超参数搜索可以尝试很多很复杂的方法来进行优化。（比如 GP-UCB 这种，一次 GP kernel regression 要对一个协方差矩阵求逆，实际上是非常慢的，在<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/33711002">为什么基于贝叶斯优化的自动调参没有大范围使用？</a>上面有一定的讨论）而 tensor optimization 这个问题，其实你把真的 tensor 程序放到机器上面跑，最慢其实也就几秒。结果你搞了个复杂的方法，要好久才能预测出来结果，那我还不如真的把程序直接放在机器上面跑呢。不过这样的好处是我们可以获得比超参数搜索多得多的数据。 第二个重大的区别是，对于 hyper-parameter optimization 来说，神经网络就是个黑盒子，我们只能根据一些概率的理论去乱调。而对于 tensor optimization 来说，我们有 AST，这是一个非常有力的信息，因为一切运算的秘密其实都隐藏在 AST 里面。 第三个区别是，tensor optimization 的任务之间其实都是相似的，可以进行 transfer learning。 在上一篇文章中我们看到，其实可能去调整的参数还是很多的，这些参数乘起来会变成非常巨大的搜索空间 \(\mathcal{S}_e\)。我们的目的就是在这个巨大的搜索空间中找到最好的 \(s\)。</p>
<h2 id="搜索框架"><a href="#搜索框架" class="headerlink" title="搜索框架"></a>搜索框架</h2><p>paper 作者提出的框架是，先搞一个 cost model \(\hat{f}(x)\)，然后用这个 \(\hat{f}(x)\) 去指导搜索出 \(s_i\)，再把 \((e_i, s_i)\) 放到机器上面跑造出 \(c_i\)，然后再去更新 \(\hat{f}(x)\)。 </p>
<p><img src="/2019/12/31/autotvm-2/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-12-30-%E4%B8%8B%E5%8D%889.28.57.png"> </p>
<p>对于这个 cost model，作者搞了两种实现。第一种是陈天奇的传统艺能——XGBoost，第二种是 TreeGRU。（顾名思义，以前的 GRU 或者 LSTM 都是一个输入，这个有多个输入，然后公式小变了一下，其实都大差不差）因为在实际的运用中，TreeGRU 实在是速度有点不行，所以根本都没有 merge 到 master 上面去，在 github 上面版本其实只有 XGBoost。 很显然，这个 cost model 的输出应该是一个预测该程序在硬件上运行的时间。对于最后的 loss，作者也实现了两种方式，第一种是传统的 regression loss：\[\sum_i(\hat{f}(x_i)-c_i)^2\] </p>
<p>第二种则是只考虑他们的相对快慢：\[\sum_{i,j}log(1+e^{-sign(c_i-c_j)(\hat{f}(x_i)-\hat{f}(x_j))})\] </p>
<p>实验表明，两种 loss 效果差不多。 因为这个搜索空间 \(\mathcal{S}_e\) 很大，我们不能枚举整个空间。这里作者使用的方法是，先在 cost model 的指导下通过模拟退火搞出一个候选集，然后再选出来一个相对比较优的集合在硬件上面进行测试，最后更新 cost model。 那么要如何定义一个相对比较优的集合呢？这个集合要同时兼顾 quality 和 diversity。作者给出的最大化式子是这样的：\[L(S)=-\sum_{s\in\mathcal{S}}\hat{f}(g(e,s))+\alpha\sum_{j=1}^m\left|{\cup_{s\in\mathcal{S}}{s_j}}\right|\] </p>
<p>这个东西如果不看代码，其实很难知道他到底在干什么东西。看过代码就知道这个 \(s\) 其实已经经过特征抽取，被平铺为一个向量了，然后 \(m\) 就是把这个平铺的向量切成 \(m\) 段。这个式子的意义就是，使得每个子段都尽可能的不一样，并且运行速度还要尽量小。 为什么这个式子要设计成这个样子，看起来不是非常奇怪吗？而且这个式子要怎么优化呢？其实原因就在于如何优化这个式子上面，这个式子是一个 submodular function，可以通过贪心求一个还算凑合的近似解，具体可以看<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/34720027">怎么理解次模函数 submodular function？</a> 整个算法的大致过程如下： </p>
<p><img src="/2019/12/31/autotvm-2/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-12-30-%E4%B8%8B%E5%8D%889.44.16.png"></p>
<h2 id="贝叶斯优化"><a href="#贝叶斯优化" class="headerlink" title="贝叶斯优化"></a>贝叶斯优化</h2><p>对于超参数搜索来说，最广为应用的方式就是贝叶斯优化。那么这个问题能不能套贝叶斯优化呢？在文章中，作者通过 bootstrap 搞出好几个 GBDT，然后通过在多个 GBDT 上面输出的预测值来采取 EI 或者 UCB 等函数，再通过上面的那种过程搜索函数的最值。虽然看起来有些奇怪（GP-UCB 那种其实是可以求出 UCB 的解析解），但总体来说其实还是符合贝叶斯优化的精神的。但是实验效果表明，用不用贝叶斯优化，效果其实都差不太多。</p>
<h2 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h2><p>他这个迁移学习模块，我觉得写的其实有点奇怪。后面做的实验也不过是对于不同 size 的卷积进行了迁移学习。然而实际上卷积运算的形式是固定的，cost model 测试的也是同一台机器上面的运算速度，所以其实相当于用的就是同一个 cost model，这个东西本身就是通用的。从这个角度看，把 AST 抽取成一个固定长度的特征就是自然而然的。 </p>
<p><img src="/2019/12/31/autotvm-2/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-12-31-%E4%B8%8B%E5%8D%882.00.14.png"> </p>
<p>对于 XGB 来说，就是简单的抽取 AST 中循环变量所代表的 touched memory 和 outer loop length，在文章中这个叫做 context feature。然而问题在于不同的算子循环变量的数量都有可能是不同的，于是在 transfer learning 中，在代码中使用了一种叫 curve sample 的技术，实际上就是采样 context features 变成一个长度固定的 context relation feature。然而为什么这样就可以提高 transfer learning 的效果，其实我也搞的不太清楚。 在 TreeGRU 中，采取的方式是将循环变量的 context feature 通过 TreeGRU fold 起来，从而得到整个 AST 的 embedding。</p>
<h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><p>中间那些不同方式的对比实验就不拿出来贴了，反正最后 state of art 是采用 rank loss 的 XGB。这里贴一个端到端的结果： </p>
<p><img src="/2019/12/31/autotvm-2/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7-2019-12-30-%E4%B8%8B%E5%8D%8810.57.23.png"> </p>
<p>从结果可以看出，优化效果非常强劲，而且越是那种非 benchmark 的网络（如 DQN），优化效果越好。 当然，这个方法并不是完美的，下一篇文章将陈述一些该方法的问题，并讲解几个 AutoTVM 方向最新的文章与成果。</p>

      </div>
      <div class="post-copyright">
    <p class="copyright-item">
      <span>Author: </span>
      <a href="https://reku1997.gitee.io">Reku</a>
    </p>
    <p class="copyright-item">
      <span>Link: </span>
      <a href="https://reku1997.gitee.io/2019/12/31/autotvm-2/">https://reku1997.gitee.io/2019/12/31/autotvm-2/</a>
    </p>
    <p class="copyright-item">
      <span>License: </span><a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>
    </p>
  </div>
      <footer class="post-footer">
        <div class="post-tags">
            <a href="/tags/AutoTVM/">AutoTVM</a>
            <a href="/tags/TVM/">TVM</a>
            </div>
        
        <nav class="post-nav"><a class="prev" href="/2020/01/02/autotvm-3/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">AutoTVM 探秘 （三）</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    <a class="next" href="/2019/12/30/autotvm-1/">
        <span class="next-text nav-default">AutoTVM 探秘（一）</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    </nav></footer>
    </article></div><div class="comments" id="comments"><div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div></div>
      </main>

      <footer id="footer" class="footer"><div class="social-links"><a href="mailto:906799571@qq.com" class="iconfont icon-email" title="email"></a>
        <a target="_blank" rel="noopener" href="https://github.com/wyc-ruiker" class="iconfont icon-github" title="github"></a>
        <a target="_blank" rel="noopener" href="https://www.zhihu.com/people/reku1997" class="iconfont icon-zhihu" title="zhihu"></a>
        <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    </div><div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" target="_blank" rel="noopener" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">&copy;2016 - 2021<span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Reku</span>
  </span>
</div>
</footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div><script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://reku1997.gitee.io/2019/12/31/autotvm-2/';
        this.page.identifier = '2019/12/31/autotvm-2/';
        this.page.title = 'AutoTVM 探秘（二）';
    };
    (function() {
    var d = document, s = d.createElement('script');

    s.src = '//reku.disqus.com/embed.js';

    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();  
  </script><script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/src/even.js?v=2.11.0"></script>
</body>
</html>
